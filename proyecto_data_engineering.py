# -*- coding: utf-8 -*-
"""Proyecto_Data_Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVEkNTthAVGhOQ10SS-eUUSw4hhnM06O
"""

#Install psycopg lybrary
#!pip install -q psycopg2
#!pip install psycopg2-binary

#Install library requests
#!pip install requests

#Install common libraries and requests.
import pandas as pd
from pathlib import Path
import psycopg2
import requests
import json

from email import message
import smtplib

from airflow import DAG
from airflow.models import DAG, Variable
import sqlalchemy
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from datetime import datetime,timedelta
from time import time
import sqlite3
from psycopg2.extras import execute_values

#Operadores
from airflow.operators.python_operator import PythonOperator
#from airflow.utils.dates import days_ago
import os

from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from requests import status_codes
from urllib.parse import quote

#argumentosd por defecto para el DAG
default_args = {
    'owner':'Mauricio Benegas',
    'start_date': datetime(2024, 1, 1),
    'retries':5,
    'retry_delay':timedelta(minutes=5)
}

Automatition = DAG( #Nombre del dag
    dag_id='Argentinian_Economy',
    default_args=default_args,
    description='Agrega información de IPC e Tipo de Cambio',
    schedule_interval="@daily",
    catchup=False
)

#Transform and clean values
def check_if_valid_data( df: pd.DataFrame ) -> bool:
#def check_if_valid_data(df: pd.DataFrame) -> bool:
  #Check is dataframe is empty
  if df.empty:
    print('No data available')
    return False

#Extraigo las APIS

# URL de la API
#Government Data ExChange Rates from 2013
api_url = "https://apis.datos.gob.ar/series/api/series/?ids=168.1_T_CAMBIOR_D_0_0_26&start_date=2013-01&limit=5000"

#Function that allow us to retrieve tha data of the api.
try:
    # Realiza la solicitud GET a la API con Requests.get
    response = requests.get(api_url)

    # Comprueba si la solicitud fue exitosa (código de respuesta 200)
    if response: #Codigo asociado al requests o status_code.
        #Parsea los datos JSON de la respuesta
        data = response.json()

        # Convierte los datos JSON a un DataFrame con Pandas
        df = pd.DataFrame(data['data'])

        # Ahora imprimo el DataFrame para revisar
        print(df)

    else:
        print(f"Error en la solicitud. Código de respuesta: {response.status_code}")

except requests.exceptions.RequestException as e:
    print(f"Error en la solicitud: {str(e)}")

#df['0'] = pd.to_datetime(df['0'])
type(df)

#df = pd.DataFrame(df, columns=['updated_date', 't_cambio_vendedor'])
#df.dropna()

df.describe

# URL de la API
api_url = "https://apis.datos.gob.ar/series/api/series/?ids=103.1_I2N_2016_M_15,431.1_EXPECTATIVANA_M_0_0_29_85&limit=5000"
#Tiene foto mensual. 	IPC Estacionales. Expectativa inflacion 12 meses. Mediana
def extraer_data(exec_date):
    try:
    # Realiza la solicitud GET a la API
        date = datetime.strptime(exec_date, '%Y-%m-%d %H')
        response = requests.get(api_url)

    # Comprueba si la solicitud fue exitosa (código de respuesta 200)
        if response:
        # Parsea los datos JSON de la respuesta
            data = response.json()

            # Convierte los datos JSON a un DataFrame
            df_monthly = pd.DataFrame(data['data'])

        # Ahora puedes trabajar con el DataFrame
            print(df_monthly)

        else:
            print(f"Error en la solicitud. Código de respuesta: {response.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"Error en la solicitud: {str(e)}")
df_monthly = pd.DataFrame(data['data'])
#Load

url="data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws"
data_base="data-engineer-database"
user="mauricioabovando2_coderhouse"
pwd='0HFN1HvQ4p'

#with open("C:/Users/Windows/Downloads/pwd_coder.txt",'r') as f:
 #   pwd= f.read()
def transformar_data(exec_date):
    try:
        date = datetime.strptime(exec_date, '%Y-%m-%d %H')
        conn = psycopg2.connect(
        host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',
        dbname=data_base,
        user=user,
        password=pwd,
        port='5439'
    )
        print("Connected to Redshift successfully!")

    except Exception as e:
        print("Unable to connect to Redshift.")
        print(e)


#cur = conn.cursor()
# Define the table name
#table_name = 'tc_argetina_vededor_v1'
# Define the columns you want to insert data into
#columns = ['updated_date', 't_cambio_vendedor'] #'id', en el api no tenemos ese ID, hay que sumarlo.
# Generate
#values = [tuple(x) for x in df.to_numpy()]
#insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s"

# Execute the INSERT statement using execute_values

#cur.execute("BEGIN")
#execute_values(cur, insert_sql, values)
#cur.execute("COMMIT")
def cargar_data(exec_date):
        
    try:
        conn = psycopg2.connect(
        host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',
        dbname=data_base,
        user=user,
        password=pwd,
        port='5439'
        )
        cur = conn.cursor()
        print(f"Conectandose a la BD en la fecha: {exec_date}")
        cur = conn.cursor()
# Define the table name
        table_name = 'tc_argetina_vededor_v1'
# Define the columns you want to insert data into
        columns = ['updated_date', 't_cambio_vendedor'] #'id', en el api no tenemos ese ID, hay que sumarlo.
# Generate
        values = [tuple(x) for x in df.to_numpy()]
        insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s"

        cur.execute("BEGIN")
        execute_values(cur, insert_sql, values)
        cur.execute("COMMIT")
    #conn.commit()
    except Exception as e:
        print(f'Error {e}')
    #print('Anything else that you feel is useful')
    conn.rollback()

   #     df_monthly

#Error invalid input syntax for type numeric: "nan"
#    df_monthly = df_monthly[df_monthly[1].notna()]


#Ingesta tabla Monthly
# Define the table name
table_name = 'inflasion_argetina'
# Define the columns you want to insert data into
columns = ['updated_date', 'ipc','inflasion'] #'id', en el api no tenemos ese ID, hay que sumarlo.
# Generate
values = [tuple(x) for x in df_monthly.to_numpy()]
insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s"

try:
        conn = psycopg2.connect(
        host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',
        dbname=data_base,
        user=user,
        password=pwd,
        port='5439'
    )
        cur = conn.cursor()
        cur.execute("BEGIN")
        execute_values(cur, insert_sql, values)
        cur.execute("COMMIT")
    #conn.commit()
except Exception as e:
    print(f'Error {e}')
    #print('Anything else that you feel is useful')
    conn.rollback()


#Envio de alertas
def enviar(ds_hour):
    try:
        x=smtplib.SMTP('smtp.gmail.com',587)
        x.starttls()
        x.login('mauricioanalyst@gmail.com','ofexhamywxyhskiq')
        subject='Dag corrio correctamente Mau'
        body_text='Excelente trabajo' #Mensaje a enviar
        message='Subject: {}\n\n{}'.format(subject,body_text)
        x.sendmail('mauricioanalyst@gmail.com','mauricioabovando2@gmail.com',message)
        print('Exito')
    except Exception as exception:
        print(exception)
        print('Failure')


#TAREAS DEL DAG

#1.EXTRACCION
task_1= PythonOperator (
    task_id='extraer_data',
    python_callable=extraer_data,
    op_args=["{{ ds }} {{ execution_date.hour }}"],
    dag=Automatition,
)
#2. Tranformacion
task_2= PythonOperator (
    task_id='transformar_data',
    python_callable=transformar_data,
    op_args=["{{ ds }} {{ execution_date.hour }}"],
    dag=Automatition,
)
#3. Envio final de info
task_3= PythonOperator (
    task_id='cargar_data',
    python_callable=cargar_data,
    op_args=["{{ ds }} {{ execution_date.hour }}"],
    dag=Automatition,
)
#Envio email
task_4= PythonOperator (
        task_id='envio_email',
        python_callable=enviar,
        op_args=["{{ ds }} {{ execution_date.hour }}"],
        dag=Automatition,
)

#definicion orden tareas
task_1 >> task_2 >> task_3 >> task_4
